{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1945fe-997d-4c4c-b82b-c0d705deb6e6",
   "metadata": {},
   "source": [
    "### Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a476f8-8390-4004-a224-64243a709a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DiffusionPipeline\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "import open_clip\n",
    "device = 'cuda'\n",
    "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "qwen = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16).cuda()\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-unclip\", torch_dtype=torch.float16).to(device)\n",
    "processor_owl = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "model_owl = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\").to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901cc44-fd06-45ed-a4f7-cba867a1e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = clip_model.to(device)\n",
    "qwen = qwen.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24de8f-7ed8-4997-9a2e-c54c68ff7f57",
   "metadata": {},
   "source": [
    "### Loading COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d425f-d873-4acf-83b7-f607b9116d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "class COCO(Dataset):\n",
    "    def __init__(self, coco_dir, split='train', transform=None):\n",
    "        self.image_dir = os.path.join(coco_dir, f\"{split}2017/\")\n",
    "        with open(os.path.join(coco_dir, f\"annotations/instances_{split}2017.json\"), 'r') as file:\n",
    "            coco = json.load(file)\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.annIm_dict = defaultdict(list)        \n",
    "        self.cat_dict = {} \n",
    "        self.annId_dict = {}\n",
    "        self.im_dict = {}\n",
    "\n",
    "        for ann in coco['annotations']:           \n",
    "            self.annIm_dict[ann['image_id']].append(ann) \n",
    "            self.annId_dict[ann['id']] = ann\n",
    "        \n",
    "        for img in coco['images']:\n",
    "            self.im_dict[img['id']] = img\n",
    "        \n",
    "        for cat in coco['categories']:\n",
    "            self.cat_dict[cat['id']] = cat\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(list(self.im_dict.keys()))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.im_dict[idx]\n",
    "        image = default_loader(os.path.join(self.image_dir, img['file_name']))\n",
    "        #display(image)\n",
    "        if self.transform is not None:\n",
    "            #print(image\n",
    "            image = self.transform(image)\n",
    "        #print(image.shape,\"::\")\n",
    "\n",
    "        targets = self.get_targets(idx)\n",
    "        #print(targets)\n",
    "        return image #targets\n",
    "        \n",
    "        \n",
    "    def get_targets(self, idx):\n",
    "        return [self.cat_dict[ann['category_id']]['name'] for ann in self.annIm_dict[idx]]\n",
    "    \n",
    "    def get_categories(self, supercategory):\n",
    "        return [self.cat_dict[cat_id]['name'] for cat_id in self.cat_dict.keys() if self.cat_dict[cat_id]['supercategory']==supercategory]\n",
    "    \n",
    "\n",
    "    def get_all_supercategories(self):\n",
    "        return {self.cat_dict[cat_id]['supercategory'] for cat_id in self.cat_dict.keys()}\n",
    "    \n",
    "    def get_spurious_supercategories(self):\n",
    "        return ['kitchen', 'food', 'vehicle',\n",
    "                'furniture', 'appliance', 'indoor',\n",
    "                'outdoor', 'electronic', 'sports',\n",
    "                'accessory', 'animal']\n",
    "    \n",
    "    def get_no_classes(self, supercategories):\n",
    "        return len([self.cat_dict[cat_id]['name'] for cat_id in self.cat_dict.keys() if self.cat_dict[cat_id]['supercategory'] in supercategories])\n",
    "    \n",
    "\n",
    "    def get_imgIds(self):\n",
    "        return list(self.im_dict.keys())\n",
    "    \n",
    "    def get_all_targets_names(self):\n",
    "        return [self.cat_dict[cat_id]['name'] for cat_id in self.cat_dict.keys()]\n",
    "    \n",
    "    def get_imgIds_by_class(self, present_classes=[], absent_classes=[]):\n",
    "        # Return images that has at least one of the present_classes, and none of the absent_classes\n",
    "        ids = []\n",
    "        for img_id in self.get_imgIds():\n",
    "            targets = self.get_targets(img_id)\n",
    "            flag = False\n",
    "            for c in present_classes:\n",
    "                if c in targets:\n",
    "                    flag = True\n",
    "                    break\n",
    "            for c in absent_classes:\n",
    "                if c in targets:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                ids.append(img_id)\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c7c33-41c3-4186-a62e-858eb8a29cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_PATH = \"COCO\"\n",
    "dset = COCO(COCO_PATH)\n",
    "supercategories = dset.get_spurious_supercategories()\n",
    "no_classes = dset.get_no_classes(supercategories)\n",
    "print(f\"Number of classes: {no_classes}\")\n",
    "for supercategory in supercategories:\n",
    "    classes = dset.get_categories(supercategory)\n",
    "    print(f\"Supercategory: {supercategory}, Classes: {classes}\")\n",
    "present = [x  for cat in dset.get_all_supercategories() for x in dset.get_categories(cat) ]\n",
    "obj_hallucination = 'boat'\n",
    "cat_spur_all = dset.get_imgIds_by_class(present_classes=present, absent_classes=[obj_hallucination])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920ff4b-cc69-4689-9949-ca293a48ba32",
   "metadata": {},
   "source": [
    "### Sort Images based on their similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1929c53-99eb-4fe6-b1e4-5500952dcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def load_and_compute_similarity(\n",
    "    clip_model,\n",
    "    exclude_indices,\n",
    "    object_text,\n",
    "    embeddings_path=\"clip_embeddings.pt\",\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \n",
    "    checkpoint = torch.load(embeddings_path)\n",
    "    clip_embeds = checkpoint[\"clip_embeds\"].to(device) \n",
    "    indices = checkpoint[\"indices\"]         \n",
    "\n",
    "    print(f\"Loaded {clip_embeds.shape[0]} embeddings.\")\n",
    "\n",
    "    if exclude_indices is not None:\n",
    "        mask = torch.isin(indices, torch.tensor(exclude_indices))\n",
    "        clip_embeds = clip_embeds[mask]\n",
    "        indices = indices[mask]\n",
    "        print(f\"Filtered down to {clip_embeds.shape[0]} embeddings after exclusion.\")\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-H-14')\n",
    "    tokens = tokenizer(object_text).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embed = clip_model.encode_text(tokens).to('cuda')\n",
    "        text_embed = F.normalize(text_embed, dim=-1)[0]  \n",
    "\n",
    "    clip_embeds = F.normalize(clip_embeds, dim=-1).to('cuda')  \n",
    "    similarities = torch.matmul(clip_embeds, text_embed)  \n",
    "    sorted_similarities, sorted_idx = torch.sort(similarities, descending=True)\n",
    "    sorted_indices = indices[sorted_idx.cpu()]\n",
    "\n",
    "    return sorted_indices, sorted_similarities\n",
    "\n",
    "    \n",
    "\n",
    "exclude_indices = []\n",
    "for i in range(len(cat_spur_all)):\n",
    "    exclude_indices.append(cat_spur_all[i])\n",
    "filtered_indices, similarities = load_and_compute_similarity(\n",
    "    clip_model,\n",
    "    exclude_indices=exclude_indices,\n",
    "    object_text=obj_hallucination,\n",
    "    embeddings_path=\"clip_embeddings.pt\"\n",
    ")\n",
    "\n",
    "# rank the top 5 matches\n",
    "top5 = torch.topk(similarities, 5)\n",
    "for score, idx in zip(top5.values, top5.indices):\n",
    "    print(f\"Index: {filtered_indices[idx]}, Similarity: {score:.4f}\")\n",
    "    image = dset[int(filtered_indices[idx])]\n",
    "    display(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d2e2c-185e-4ec3-9502-e4ff7206654f",
   "metadata": {},
   "source": [
    "### Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de08b7c-49c0-4ac3-8e3c-8aae4715159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_obj_owlvit(image: Image.Image, score_threshold=0.1):\n",
    "    texts = [[obj_hallucination]]  # supports multiple labels\n",
    "    inputs = processor_owl(text=texts, images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_owl(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image.size[::-1]])  # (H, W)\n",
    "    results = processor_owl.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=score_threshold)[0]\n",
    "\n",
    "    for score, label in zip(results[\"scores\"], results[\"labels\"]):\n",
    "        if label == 0 and score > score_threshold:  \n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427531f3-6a85-4d36-86f7-34012a3b47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def get_obj_owlvit(image: Image.Image, score_threshold=0.1):\n",
    "    texts = [[obj_hallucination]]  # supports multiple labels\n",
    "    inputs = processor_owl(text=texts, images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_owl(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(\"cuda\")  \n",
    "    results = processor_owl.post_process_object_detection(\n",
    "        outputs=outputs,\n",
    "        target_sizes=target_sizes,\n",
    "        threshold=score_threshold\n",
    "    )[0]\n",
    "\n",
    "    boxes = results[\"boxes\"]\n",
    "    scores = results[\"scores\"]\n",
    "    labels = results[\"labels\"]\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        return None\n",
    "\n",
    "    top_idx = scores.argmax().item()\n",
    "    box = boxes[top_idx].tolist() \n",
    "    obj_crop = image.crop(box)\n",
    "    return obj_crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b6ddd-3cc7-4e14-a2be-745ac723fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\n",
    "    \"Do you see a {obj} in the image?\",\n",
    "    \"Is there a {obj} here?\",\n",
    "    \"Does the image contain a {obj}?\",\n",
    "    \"Can you find a {obj} in this picture?\",\n",
    "    \"Would you say there's a {obj} here?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e846ad-dc78-442a-b433-364ad9c5d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipToQwenProjector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4096, 3584),\n",
    "            nn.GELU(),    \n",
    "            nn.LayerNorm(3584)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603532f2-4238-4023-aa3b-e085235a836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "model = ClipToQwenProjector().cuda()\n",
    "checkpoint = torch.load('Reverese.pt')\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017993b-49db-4bd2-b0b3-3613038db733",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in qwen.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pipe.vae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pipe.unet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pipe.text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aece72-4099-4f47-8958-dd8351c6da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_qwen(prompt, img,model=None):\n",
    "    with torch.no_grad():\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": img\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }  \n",
    "    \n",
    "        ]\n",
    "    \n",
    "        # Preparation for inference\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "        # Inference: Generation of the output\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        \n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38114e-c4a7-4309-9566-75ddd978a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "def get_clip_embedding(image):\n",
    "    inputs = torch.stack([clip_preprocess(image)]).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(inputs)[0]\n",
    "    return embedding\n",
    "\n",
    "def get_qwen_inputs(prompt, image, clip_embed):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ]}\n",
    "    ]\n",
    "    mean = model(clip_embed)\n",
    "    qwen_tokens = mean.repeat(64, 1)\n",
    "\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cuda')\n",
    "\n",
    "    inputs_embeds = qwen.get_input_embeddings()(inputs['input_ids'])\n",
    "\n",
    "    n_image_tokens = (inputs['input_ids'] == qwen.config.image_token_id).sum().item()\n",
    "    n_image_features = qwen_tokens.shape[0]\n",
    "\n",
    "    if n_image_tokens != n_image_features:\n",
    "        raise ValueError(\n",
    "            f\"Image features and image tokens do not match: tokens={n_image_tokens}, features={n_image_features}\"\n",
    "        )\n",
    "\n",
    "    image_mask = (inputs['input_ids'] == qwen.config.image_token_id)\\\n",
    "        .unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n",
    "\n",
    "    image_embeds = qwen_tokens.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "    inputs['inputs_embeds'] = inputs_embeds\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def get_qwen_probabilities(inputs):\n",
    "    logits = qwen(**inputs).logits.float()\n",
    "    logits_step = logits[:, -1, :]\n",
    "    probs = torch.softmax(logits_step, dim=-1)\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "def compute_loss(log_prob_yes, clip_embed, text_embedding, clip_embed_orig, clip_embed_gen=None):\n",
    "    sim1 = F.cosine_similarity(clip_embed, text_embedding)\n",
    "    sim2 = F.mse_loss(clip_embed, clip_embed_orig[0])\n",
    "    sim3 = 0\n",
    "    if clip_embed_gen is not None:\n",
    "        sim3 = F.cosine_similarity(clip_embed, clip_embed_gen.unsqueeze(0))\n",
    "        return log_prob_yes + 5 * sim1 + 5 * sim2 + 5 * sim3, sim1, sim2, sim3\n",
    "    else:\n",
    "        return log_prob_yes + 5 * sim1 + 5 * sim2, sim1, sim2, sim3\n",
    "\n",
    "\n",
    "def generate_and_validate_image(pipe, prompt, clip_embed, qwen, processor, yes_id, no_id):\n",
    "    result = pipe(\n",
    "        negative_prompt=\"low quality, ugly, unrealistic\",\n",
    "        image_embeds=clip_embed.unsqueeze(0),\n",
    "        guidance_scale=10\n",
    "    )\n",
    "    generated = result.images[0]\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    output = ask_qwen(prompt, generated, qwen)\n",
    "\n",
    "    if output[0].lower().startswith(\"yes\"):\n",
    "        plt.imshow(generated)\n",
    "        plt.show()\n",
    "        return generated, output\n",
    "    return None, output\n",
    "\n",
    "def get_qwen_loss(probs,yes_id, no_id):\n",
    "    \n",
    "    prob_yes = probs[0, yes_id]\n",
    "    prob_no = probs[0, no_id]\n",
    "    log_prob_yes = -torch.log(prob_yes + 1e-8)\n",
    "\n",
    "    return log_prob_yes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main_loop(\n",
    "    dset, object_hull\n",
    "):\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-H-14')\n",
    "\n",
    "    for i in range(0, len(filtered_indices)):\n",
    "        prompt = random.choice(templates).format(obj=object_hull)\n",
    "        image = dset[int(filtered_indices[i])].resize((224, 224))\n",
    "        display(image)\n",
    "\n",
    "        clip_embed = get_clip_embedding(image)\n",
    "        output = ask_qwen(prompt, image, qwen)\n",
    "        if output[0].lower().startswith(\"yes\"):\n",
    "            continue\n",
    "\n",
    "        clip_embed = nn.Parameter(clip_embed)\n",
    "        optimizer = torch.optim.SGD([clip_embed], lr=1)\n",
    "        clip_embed_orig = clip_embed.clone().detach().unsqueeze(0)\n",
    "        clip_embed_gen = None\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            prompt = random.choice(templates).format(obj=obj_hallucination)\n",
    "            inputs = get_qwen_inputs(prompt, image, clip_embed)\n",
    "            probs = get_qwen_probabilities(inputs)\n",
    "            yes_id = processor.tokenizer(\"Yes\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "            no_id = processor.tokenizer(\"No\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "            log_prob_yes = get_qwen_loss(probs,yes_id,no_id)\n",
    "            tokens = tokenizer(obj_hallucination).to('cuda')\n",
    "            text_embedding = clip_model.encode_text(tokens).detach()\n",
    "            loss, sim1, sim2, sim3 = compute_loss(\n",
    "                log_prob_yes, clip_embed, text_embedding, clip_embed_orig, clip_embed_gen\n",
    "            )\n",
    "            print(sim1, sim2, sim3)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            generated_ids = qwen.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1,\n",
    "                do_sample=False,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "            gen_probs = torch.softmax(generated_ids.scores[0], dim=-1)\n",
    "\n",
    "            print(\"Generated probs:\")\n",
    "            print(\"  Yes:\", gen_probs[0, yes_id].item())\n",
    "            print(\"  No :\", gen_probs[0, no_id].item())\n",
    "\n",
    "            if gen_probs[0, yes_id] > gen_probs[0, no_id]:\n",
    "                generated, output = generate_and_validate_image(\n",
    "                    pipe, prompt, clip_embed.half(), qwen, processor, yes_id, no_id\n",
    "                )\n",
    "                if generated is not None:\n",
    "                    if contains_obj_owlvit(generated, 0.3):\n",
    "                        crop_gen = get_obj_owlvit(generated)\n",
    "                        clip_inputs_gen = torch.stack([clip_preprocess(crop_gen)]).to('cuda')\n",
    "                        with torch.no_grad():\n",
    "                            clip_embed_gen = clip_model.encode_image(clip_inputs_gen).float()[0].detach()\n",
    "                        print(f\"It contains a  {obj_hallucination}\")\n",
    "                    else:\n",
    "                        print(f\"It does not contain a {obj_hallucination}\")\n",
    "                   \n",
    "\n",
    "\n",
    "\n",
    "main_loop(dset, obj_hallucination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42096da5-a5ad-44e6-85c8-9d7f2534f43c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
